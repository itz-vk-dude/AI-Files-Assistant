Here is a complete and clear `README.md` for your **PromptChain AI** backend project with RLHF support, numeric total merging, and multi-model QA:

---

```markdown
# ğŸ¤– PromptChain AI â€“ Backend

A powerful AI assistant backend for summarizing files from Google Drive, answering questions using LLMs, scoring answers with a reward model, and logging chat history. Supports multi-model querying, chunk-based context search, numeric answer merging, and RLHF-style feedback.

---

## ğŸš€ Features

- âœ… Load all file types from a **Google Drive folder**
- ğŸ“„ Extract text from `.pdf`, `.docx`, `.xlsx`, `.csv`, `.txt`, `.xml`, etc.
- ğŸ§  Summarize using **Ollama** (`llama3:8b`)
- â“ Ask questions using:
  - ğŸ” OpenRouter (e.g., Mistral)
  - ğŸ” Ollama (e.g., DeepSeek Math)
- ğŸ§  Top-k chunk retrieval with sentence embeddings
- ğŸ“Š Merge numeric totals from multiple file chunks
- ğŸ’¬ Chat history saved per folder
- ğŸ† RLHF support with reward model scoring
- â­ Manual feedback rating (`ğŸ‘` / `ğŸ‘`)
- ğŸ” Fine-tune reward model from logs

---

## ğŸ“¦ Project Structure

```

backend/
â”‚
â”œâ”€â”€ main.py                    # FastAPI backend entry point
â”œâ”€â”€ qa\_engine.py               # Multi-model hybrid QA engine
â”œâ”€â”€ summarizer.py              # Ollama-based summarization
â”œâ”€â”€ file\_processor.py          # Text extraction from various formats
â”œâ”€â”€ drive\_handler.py           # Download files from Google Drive
â”œâ”€â”€ chat\_history.py            # Per-folder chat log handling
â”‚
â”œâ”€â”€ reward/
â”‚   â”œâ”€â”€ reward\_model.py        # Local scoring wrapper
â”‚   â”œâ”€â”€ reward\_model\_trainer.py # RLHF-style fine-tuner
â”‚
â”œâ”€â”€ memory\_store.py            # In-memory file+chunk storage
â”œâ”€â”€ embedding\_utils.py         # Chunk embedding utils
â”‚
â”œâ”€â”€ chat\_logs/                 # All chat histories per folder
â”œâ”€â”€ feedback/                  # Feedback logs for training
â””â”€â”€ reward\_model\_output/       # Fine-tuned reward model directory

````

---

## âš™ï¸ Setup Instructions

### 1. ğŸ“ Install Python requirements

```bash
pip install -r requirements.txt
````

Required:

* `fastapi`, `uvicorn`
* `scikit-learn`, `requests`
* `sentence-transformers`, `python-docx`, `PyMuPDF`, `openpyxl`, `pandas`
* `ollama`, `google-api-python-client`, `pydantic`

---

### 2. ğŸš€ Start the backend server

```bash
cd backend
uvicorn main:app --reload
```

Server runs at:
ğŸ“¡ [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

### 3. ğŸ”— Frontend integration (CORS)

Make sure your frontend is served at:

* [http://localhost:5173](http://localhost:5173)
* [http://127.0.0.1:5173](http://127.0.0.1:5173)

These are whitelisted for CORS in `main.py`.

---

## ğŸ” API Endpoints

| Route                        | Description                                    |
| ---------------------------- | ---------------------------------------------- |
| `GET /`                      | Health check                                   |
| `GET /load_drive`            | Load and summarize all files from Drive folder |
| `GET /load_history`          | Load chat history for a folder                 |
| `POST /ask`                  | Ask a question (hybrid LLM)                    |
| `POST /ask_best`             | Ask question with RLHF reward selection        |
| `POST /rate_response`        | Rate an answer with ğŸ‘/ğŸ‘                      |
| `GET /export_training_data`  | Export chat logs to `training_data.jsonl`      |
| `GET /finetune_reward_model` | Trigger fine-tuning of reward model            |

---

## âœ… RLHF Workflow

1. ğŸ§  Ask questions via `/ask_best`
2. ğŸ‘ / ğŸ‘ answers via `/rate_response`
3. ğŸ“¤ Export logs: `/export_training_data`
4. ğŸ§ª Fine-tune reward model: `/finetune_reward_model`
5. ğŸ” System now scores future answers better!

---

## ğŸ¤ Feedback Logging

Each feedback is saved to:

```
feedback/{folder_id}_ratings.jsonl

{
  "timestamp": "2025-07-18T10:00:00Z",
  "folder_id": "1W7nZ...TQ",
  "question": "...",
  "answer": "...",
  "score": 1
}
```

---

## ğŸ“Œ Notes

* Automatically merges numeric totals when asking "total" questions.
* Automatically shortens verbose multi-source answers.
* Works offline via Ollama, or hybrid with OpenRouter if connected.

---

## ğŸ“£ Credits

Built by Vasantha Kumar S, Sarath KR,Yamini T,Ruthrabala S, Gayathri N, Sheerin Rizwana Y.with â¤ï¸
Powered by FastAPI + Ollama + OpenRouter + HuggingFace

---
